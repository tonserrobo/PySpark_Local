# PySpark Local: A simple guide to getting started using PySpark locally

Simple Repo for running PySpark locally using VS Code, Jupyter Notebooks, Jetbrains or any other local IDE. NOTE, this repo is intended for Ulster University MSc Data Science students (Big data technologies COM739). However, if you find it useful then please feel free to use it.

## Installing PySpark locally with Notebooks

As we have seen over the past few weeks, there is an issue on google Colab due to Java versions. Its not entirely fair to point the blame exclusively at Java, as the incompatibility comes from Java - Python V3.13 and PySpark 4.0.1. I've discovered that it works best if we use Java v11, Python 3.11 and we can continue to use PySpark 3.5.1. I originally tried to avoid introducing virtual environments as it introduces unnesseucary complexity; however this is the real world of software engineering. The method i prefer to use when managing complex dependency relationships is UV. To setup PySpark locally and ensure the dependencies are managed we can go through the following process

You may be wondering what the other files are within this repo. Here is an overview of the repo including brief introduction to the different file types (many of these are created by UV for the purpose of managing our environment).

| File    | Directory | Description                                                                                                          |
| :------ | :-------- | :------------------------------------------------------------------------------------------------------------------- |
| uv.lock | Root      | locks your Python project's dependencies to specific versions to ensure reproducible and cross-platform environments |

| pyproject.toml | Root | This is the project detail (like requirements.txt). It is generated by UV to manage project metadata, dependencies, build systems, and other specific configurations |

- README.md
- pyproject.toml
- LICENSE
- .python-version
- data
- docs
- test

### Installing UV

You will first need to install UV. This is fundamentally a package manager like PIP (python) however has the added benefit of allowing us to manage virtual environments and python versions as we would any other package.

```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
```

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

You can find more documentation on getting started with UV (and installation) [here](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_1)

There is more information and some useful tutorials on what UV is and some common cmds when using it here. Some of them are decent but i've also attached specific notes on UV in general. Only review these as required, they are not that important but useful for getting and understanding of the tool and any issues that might arise.

- [UV Docs](https://docs.astral.sh/uv/): official documentation
- [UV introduction - Tech with Tim](https://www.youtube.com/watch?v=6pttmsBSi8M&ab_channel=TechWithTim): Decent enough overview of what UV is and how we can use it.
- [UV more details - Another youtube tutorial](https://www.youtube.com/watch?v=qh98qOND6MI&ab_channel=ArjanCodes)

### Setting up venv

With UV installed, we can proceed to setting up the virtual environment and initializing spark locally (this should work for VS code, jupyter notebooks and any other local development environment). We can do this in colab also if we want although its not really necessary.

1. Install UV (we should have this done already. we can check by running the cmd uv --version)
2. Clone UV repo (https://github.com/tonserrobo/PySpark_Local.git). Make sure you open a terminal within this directory.
3. Setup UV virtual environment (cmd: uv venv). This will create a python 3.11 venv
4. activate the venv (cmd: .venv\Scripts\activate). This will activate the venv and allow us to install the dependencies.
5. run UV sync to pull all the required dependencies (cmd: uv sync). This will install all the dependencies to the venv

NOTE: i've placed the package files within blackboard also, so this essentially replaces step 2 if you dont want to clone the github repo i created. Lastly, when we are finished working in our venv, you can deactivate it by typing the cmd 'deactivate'.

Once we have that done, we can then import pyspark and initlise our spark session

```Python

import sys, os, findspark
from pyspark.sql import SparkSession

os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

findspark.init()

spark = SparkSession.builder.appName("SparkTest").getOrCreate()

print(f"findspark version: {findspark.__version__}")
print(f"pyspark version: {spark.version}")

```

This should output the following:

findspark version: 2.0.1
pyspark version: 3.5.7

## Common errors

I've put together a help document in `Docs > common_pitfalls.md` to help you identify and resolve any errors which might be coming up during the installation and sync process. These errors don't go into PySpark specific problems, just virtual environment management related to PySpark per our use case.
