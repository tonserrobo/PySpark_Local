{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0679a797",
   "metadata": {},
   "source": [
    "# Spark Local Installation Guide\n",
    "\n",
    "This notebook provides two methods for setting up Apache Spark in a local environment (e.g., Google Colab, local Jupyter notebook, or similar platforms). \n",
    "\n",
    "**Apache Spark** is an open-source unified analytics engine for large-scale data processing. **PySpark** is the Python API for Apache Spark, allowing you to perform real-time, large-scale data processing in a distributed environment using Python.\n",
    "\n",
    "## Prerequisites\n",
    "- Python environment (Jupyter, Colab, etc.)\n",
    "- Internet connection (for downloading packages/binaries)\n",
    "- Basic understanding of Python\n",
    "\n",
    "## Two Installation Methods\n",
    "\n",
    "1. **Method 1: Manual Installation with Binaries** - Download and extract Spark binaries manually (useful when you need a specific Spark version or have pre-downloaded files)\n",
    "2. **Method 2: Simple pip Installation** - Install PySpark directly via pip (recommended for quick setup and most use cases)\n",
    "\n",
    "---\n",
    "\n",
    "## Method 1: Manual Installation with Binaries\n",
    "\n",
    "This method involves downloading the Spark binaries and setting up the environment manually. This is useful when:\n",
    "- You need a specific version of Spark\n",
    "- You want to upload pre-downloaded binaries to avoid long download times\n",
    "- You're working in an environment like Google Colab with ephemeral storage\n",
    "\n",
    "### Step 1: Clean Previous Installation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment variables to ensure clean installation\n",
    "# This removes any existing Spark environment variables\n",
    "import os, sys, shutil, glob\n",
    "\n",
    "for var in [\"SPARK_HOME\", \"PYSPARK_SUBMIT_ARGS\", \"PYSPARK_PYTHON\", \"PYSPARK_DRIVER_PYTHON\"]:\n",
    "    os.environ.pop(var, None)\n",
    "\n",
    "# Optional: remove any old spark folders (for Google Colab)\n",
    "for p in glob.glob(\"/content/spark-*\"):\n",
    "    try:\n",
    "        shutil.rmtree(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Environment cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n7h7e6ydwe",
   "metadata": {},
   "source": [
    "### Step 2: Install Java\n",
    "\n",
    "Spark requires Java to run. We'll install OpenJDK 11 (a free and open-source implementation of Java).\n",
    "\n",
    "**Note:** This command is for Linux-based systems (like Google Colab). For Windows or Mac, you'll need to install Java manually from the Java website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60yljsjvlqr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Java (OpenJDK 11) - This is required for Spark to run\n",
    "# The -qq flag suppresses output, > /dev/null redirects output to null\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "print(\"Java installed successfully!\")\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fu0euu0sng",
   "metadata": {},
   "source": [
    "### Step 3: Download and Extract Spark Binaries\n",
    "\n",
    "There are two options here:\n",
    "\n",
    "**Option A:** Download directly from Apache servers (can take a while)\n",
    "- Uncomment the `!wget` line below to download\n",
    "\n",
    "**Option B:** Upload pre-downloaded binaries\n",
    "- Download `spark-3.5.1-bin-hadoop3.tgz` to your computer from https://archive.apache.org/dist/spark/spark-3.5.1/\n",
    "- In Google Colab: Click the folder icon on the left sidebar\n",
    "- Upload the `.tgz` file to the `/content/` directory\n",
    "- Then run the extraction command below\n",
    "\n",
    "We're using **Spark 3.5.1**, which is the latest stable release as of January 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4kbgasfbmlf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Download Spark 3.5.1 from Apache (uncomment the line below if needed)\n",
    "# This can take several minutes depending on your internet connection\n",
    "# !wget -q https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# Extract Spark 3.5.1 binaries\n",
    "# This assumes the .tgz file is in the current directory\n",
    "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "print(\"Spark binaries extracted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4v193hdzejs",
   "metadata": {},
   "source": [
    "### Step 4: Install PySpark and Findspark Python Packages\n",
    "\n",
    "- **PySpark:** Python API for Apache Spark\n",
    "- **Findspark:** Helper library to locate Spark installation and initialize it\n",
    "\n",
    "We install these via pip to get the Python bindings for Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6wd26pg1zz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark and Findspark packages\n",
    "# -q flag makes the installation quiet (less verbose output)\n",
    "!pip install -q pyspark findspark\n",
    "\n",
    "# Alternative: Install a specific version of PySpark (if needed)\n",
    "# !pip install -q pyspark==3.5.1\n",
    "\n",
    "print(\"PySpark and Findspark installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6j6e3oypwai",
   "metadata": {},
   "source": [
    "### Step 5: Initialize Spark Session\n",
    "\n",
    "A **SparkSession** is the entry point to using Spark functionality. It allows you to:\n",
    "- Create DataFrames\n",
    "- Read and write data\n",
    "- Execute SQL queries\n",
    "- Manage Spark configuration\n",
    "\n",
    "By setting `.master(\"local[*]\")`, we configure Spark to run locally using all available CPU cores. The `*` means use all available cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parly847jkq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session\n",
    "# - master(\"local[*]\"): Run Spark locally with all available CPU cores\n",
    "# - appName(): Give your Spark application a name (useful for monitoring)\n",
    "# - getOrCreate(): Get existing session or create new one if none exists\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"LocalSparkSetup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify installation by printing Spark version\n",
    "print(\"✅ Spark installation successful!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# You can also access SparkContext (lower-level API) through the session\n",
    "sc = spark.sparkContext\n",
    "print(f\"SparkContext available: {sc is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eoocosqermj",
   "metadata": {},
   "source": [
    "### Step 6: Test the Installation\n",
    "\n",
    "Let's test if Spark is working correctly by creating a simple DataFrame and performing a basic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44qjqscaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test DataFrame\n",
    "test_data = [\n",
    "    (\"Alice\", 25, \"Engineer\"),\n",
    "    (\"Bob\", 30, \"Data Scientist\"),\n",
    "    (\"Charlie\", 35, \"Manager\")\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Age\", \"Job\"]\n",
    "\n",
    "# Create DataFrame from test data\n",
    "df = spark.createDataFrame(data=test_data, schema=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Test DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Perform a simple operation - filter ages > 25\n",
    "print(\"\\nFiltered DataFrame (Age > 25):\")\n",
    "df.filter(df.Age > 25).show()\n",
    "\n",
    "print(\"\\n✅ All tests passed! Spark is working correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting: If Method 1 didn't work, run this cell to reset everything\n",
    "\n",
    "import os, sys, shutil, glob\n",
    "\n",
    "# Remove all Spark environment variables\n",
    "for var in [\"SPARK_HOME\", \"PYSPARK_SUBMIT_ARGS\", \"PYSPARK_PYTHON\", \"PYSPARK_DRIVER_PYTHON\"]:\n",
    "    os.environ.pop(var, None)\n",
    "\n",
    "# Remove any old spark folders\n",
    "for p in glob.glob(\"/content/spark-*\"):\n",
    "    try:\n",
    "        shutil.rmtree(p)\n",
    "        print(f\"Removed: {p}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not remove {p}: {e}\")\n",
    "\n",
    "print(\"\\n✅ Environment reset complete! Try running the installation steps again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32627d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Method 2: Simple pip Installation (Recommended)\n",
    "\n",
    "This is the **simplest and quickest method** for setting up Spark. It's ideal when:\n",
    "- You want a quick setup without dealing with binaries\n",
    "- You're okay with the latest stable PySpark version from PyPI\n",
    "- You don't need a specific Spark distribution\n",
    "\n",
    "This method only requires installing the PySpark package via pip - no need for Java installation or binary downloads!\n",
    "\n",
    "### Step 1: Install PySpark Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m6f0fn1ynpp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark via pip\n",
    "# This will automatically download and install PySpark and all its dependencies\n",
    "!pip install -q pyspark\n",
    "\n",
    "# Optional: Install findspark if you need it for more complex setups\n",
    "# !pip install -q findspark\n",
    "\n",
    "print(\"✅ PySpark installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6b0dwahu",
   "metadata": {},
   "source": [
    "### Step 2: Create and Initialize Spark Session\n",
    "\n",
    "That's it! With PySpark installed, you can now directly create a Spark session and start using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1rqi48hdxb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PipSparkSetup\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify installation\n",
    "print(\"✅ Spark session created successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Get SparkContext for RDD operations\n",
    "sc = spark.sparkContext\n",
    "print(f\"SparkContext ID: {sc.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bgwpx2v0m7p",
   "metadata": {},
   "source": [
    "### Step 3: Test Your Installation\n",
    "\n",
    "Run a quick test to ensure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uau4gaouxqs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Create a DataFrame\n",
    "data = [(\"Spark\", 2014), (\"Python\", 1991), (\"Scala\", 2003), (\"Java\", 1995)]\n",
    "columns = [\"Language\", \"Year\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Test 1: DataFrame Creation\")\n",
    "df.show()\n",
    "\n",
    "# Test 2: Perform transformations\n",
    "print(\"\\nTest 2: Filter languages created after 2000\")\n",
    "df.filter(df.Year > 2000).show()\n",
    "\n",
    "# Test 3: Use RDD (low-level API)\n",
    "print(\"\\nTest 3: RDD Operations\")\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "squared = rdd.map(lambda x: x**2)\n",
    "print(f\"Original: {rdd.collect()}\")\n",
    "print(f\"Squared: {squared.collect()}\")\n",
    "\n",
    "print(\"\\n✅ All tests passed! Spark is fully functional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gs5v4f4eaii",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "### Which Method Should You Use?\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Method 1** | • Specific Spark version control<br>• Can work offline with pre-downloaded binaries<br>• Full Spark distribution | • More complex setup<br>• Requires Java installation<br>• Larger download size | • When you need a specific Spark version<br>• When working with large-scale projects<br>• When you need full Spark features |\n",
    "| **Method 2** | • Quick and simple<br>• Minimal dependencies<br>• Easy to maintain | • Uses latest PyPI version<br>• Less control over Spark version | • Quick prototyping<br>• Learning PySpark<br>• Most data analysis tasks |\n",
    "\n",
    "### Key Concepts to Remember\n",
    "\n",
    "1. **SparkSession**: The entry point for all Spark functionality\n",
    "2. **SparkContext**: Lower-level API accessed via `spark.sparkContext`\n",
    "3. **Local Mode**: Running Spark on a single machine (what we're doing here)\n",
    "4. **Distributed Mode**: Running Spark on a cluster (covered in advanced topics)\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "1. **Java Not Found Error**\n",
    "   - Solution: Make sure Java is installed (required for Method 1)\n",
    "   - Check with: `!java -version`\n",
    "\n",
    "2. **Module Not Found: pyspark**\n",
    "   - Solution: Run `!pip install pyspark`\n",
    "   \n",
    "3. **Port Already in Use**\n",
    "   - Solution: Restart your kernel/runtime and try again\n",
    "\n",
    "4. **Out of Memory Errors**\n",
    "   - Solution: Reduce data size or configure Spark memory settings\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [PySpark Official Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [PySpark by Examples](https://sparkbyexamples.com/pyspark-tutorial/)\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [PySpark Cheat Sheet](https://sparkbyexamples.com/pyspark/pyspark-sql-cheat-sheet/)\n",
    "\n",
    "### Stopping Spark Session\n",
    "\n",
    "When you're done working with Spark, it's good practice to stop the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m9fn3ig546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when you're done\n",
    "# This releases resources and cleans up\n",
    "spark.stop()\n",
    "\n",
    "print(\"✅ Spark session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
