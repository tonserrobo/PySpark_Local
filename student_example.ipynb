{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Student Example Notebook\n",
    "\n",
    "This notebook demonstrates the simple one-line setup for PySpark.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "Just run the cell below to initialize your PySpark environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'get_ipython' could not be imported from 'unknown location'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# One-line initialization - this does everything!\n",
    "from pyspark_local import initialize_pyspark\n",
    "spark = initialize_pyspark(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "The `initialize_pyspark()` function just:\n",
    "1. ✓ Checked that Java is installed\n",
    "2. ✓ Verified all PySpark dependencies\n",
    "3. ✓ Created a Spark session configured for local use\n",
    "4. ✓ Ran validation tests to ensure everything works\n",
    "\n",
    "Now you can start working with Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Range Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple range DataFrame\n",
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Create DataFrame from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from Python data\n",
    "data = [\n",
    "    ('James', '', 'Smith', '1991-04-01', 'M', 3000),\n",
    "    ('Michael', 'Rose', '', '2000-05-19', 'M', 4000),\n",
    "    ('Robert', '', 'Williams', '1978-09-05', 'M', 4000),\n",
    "    ('Maria', 'Anne', 'Jones', '1967-12-01', 'F', 4000),\n",
    "    ('Jen', 'Mary', 'Brown', '1980-02-17', 'F', 5000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "high_earners = df.filter(df.salary > 3500)\n",
    "high_earners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"firstname\", \"lastname\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by and aggregate\n",
    "df.groupBy(\"gender\").avg(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Run SQL queries\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT gender, AVG(salary) as avg_salary, COUNT(*) as count\n",
    "    FROM employees\n",
    "    GROUP BY gender\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Reading JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file (if available)\n",
    "try:\n",
    "    json_df = spark.read.json(\"data/people.json\")\n",
    "    json_df.show()\n",
    "    json_df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Note: Sample data file not found. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Your Spark Application\n",
    "\n",
    "While your Spark session is running, you can monitor it at:\n",
    "- **Spark UI**: http://localhost:4040\n",
    "\n",
    "This shows you:\n",
    "- Jobs and stages\n",
    "- Storage\n",
    "- Environment settings\n",
    "- Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When You're Done\n",
    "\n",
    "Always remember to stop your Spark session when you're finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Manual Setup (for reference)\n",
    "\n",
    "If you want more control, you can use the functions individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this if you already initialized above!\n",
    "\n",
    "from pyspark_local import check_environment, create_spark_session, run_validation_tests\n",
    "\n",
    "# 1. Check environment manually\n",
    "env = check_environment()\n",
    "print(env)\n",
    "\n",
    "# 2. Create session with custom config\n",
    "spark = create_spark_session(\n",
    "    app_name=\"MyCustomApp\",\n",
    "    master=\"local[2]\",  # Use only 2 cores\n",
    "    log_level=\"WARN\"\n",
    ")\n",
    "\n",
    "# 3. Run validation tests\n",
    "results = run_validation_tests(spark, verbose=True)\n",
    "\n",
    "# Remember to stop when done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_Local (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
